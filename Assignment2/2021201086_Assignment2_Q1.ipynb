{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 1\n",
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Clustering`.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analysis in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of other cells.\n",
    "- No inbuilt functions to be used until specified\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>_Assignment2_Q1.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdEXGGO0AqkW",
    "outputId": "119fb7e5-3656-4bdb-fcfa-2e33afe2d0d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/soumodiptab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')    \n",
    "# if u r facing issues while importing nltk, please uncomment above line and run\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5N2FkdFMxixH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 105 kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 130 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk in /home/soumodiptab/.local/lib/python3.8/site-packages (from sentence-transformers) (3.6.7)\n",
      "Requirement already satisfied: numpy in /home/soumodiptab/.local/lib/python3.8/site-packages (from sentence-transformers) (1.22.1)\n",
      "Requirement already satisfied: scikit-learn in /home/soumodiptab/.local/lib/python3.8/site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /home/soumodiptab/.local/lib/python3.8/site-packages (from sentence-transformers) (1.7.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 30 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.11.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 49 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.10.2-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |█▍                              | 36.9 MB 34 kB/s eta 6:42:2806:41:520^C\n",
      "\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/soumodiptab/courses/SMAI/Assignment2/2021201086_Assignment2_Q1.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/courses/SMAI/Assignment2/2021201086_Assignment2_Q1.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install sentence-transformers\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/soumodiptab/courses/SMAI/Assignment2/2021201086_Assignment2_Q1.ipynb#ch0000002vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-1irtUhxlp2"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "\n",
    "*   Try to explore the dataset and first understand\n",
    "*   Steps while processing the dataset:\n",
    "\n",
    "1.   Load the dataset\n",
    ">> The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: train and test. Here, we only use train part of the dataset as we don't need any training.\n",
    "\n",
    "2.   pre-processing of the dataset\n",
    ">>   A set of basic pre-processing steps are given below, if you can do it better, it is appreciable\n",
    "3.   Trying to obtain the embeddings for the text. \n",
    ">> Here, we used bert model to obtain the embeddings, if you want to use anyother sentence/word embeddings (ELMo,universal sentence encoder, or other bert models) you can use it, but not mandatorily change it)\n",
    "\n",
    "PS: You need not completely understand how bert works. If you are interested, few links will be mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqkn6iSNmEo1"
   },
   "outputs": [],
   "source": [
    "# loading of dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# print(list(newsgroups_train))\n",
    "#['data', 'filenames', 'target_names', 'target', 'DESCR']\n",
    "# all we require for our task is data and target. \n",
    "#target_names describe the different groups present (which are 20) all over the dataset\n",
    "\n",
    "# print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNRbIfsX01hD"
   },
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp8UL8lP5U2k"
   },
   "outputs": [],
   "source": [
    "#preprocessing of sentences and the article\n",
    "\n",
    "def remove_punct(text):\n",
    "  text = re.sub('[^a-zA-Z0-9 ]+','', text)\n",
    "  return text\n",
    "\n",
    "def remove_urls(text):\n",
    "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "  return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_tag(text):   \n",
    "  text=' '.join(text)\n",
    "  html_pattern = re.compile('<.*?>')\n",
    "  return html_pattern.sub(r'', text)\n",
    "\n",
    "def pre_process_sentence(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  sentence = remove_punct(remove_urls(sentence))\n",
    "  return sentence\n",
    "\n",
    "def pre_process_article(article):\n",
    "  article = str(article).replace(\"\\n\", '')\n",
    "  article = sent_tokenize(article)\n",
    "  sentences = []\n",
    "  for each in article:\n",
    "    if len(each.split(\":\")) > 1:\n",
    "      continue\n",
    "    sentences.append(pre_process_sentence(each))\n",
    "  return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72IcGYs980zK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_review_embedding(article):\n",
    "  sentences = pre_process_article(article)\n",
    "\n",
    "  #here review(input) has to be a list of sentences\n",
    "  #use suitable embeddings to get an embedding for the whole review\n",
    "  #usage of sentence embeddings is recommended\n",
    "\n",
    "  sentence_embeddings = bert_model.encode(sentences)\n",
    "\n",
    "  # take average of all sentence embeddings to obtain a review embedding \n",
    "  review_embedding = np.zeros(768)\n",
    "  for each in sentence_embeddings:\n",
    "    review_embedding = np.add(np.array(each), review_embedding)\n",
    "\n",
    "  return review_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbkl4Ux0JPi2"
   },
   "outputs": [],
   "source": [
    "X, y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkaKG4WW9eXx"
   },
   "outputs": [],
   "source": [
    "# data visualization \n",
    "\n",
    "# Try to visualise the points from all the domains and try to visualise them \n",
    "# hint: you can use PCA \n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZH6gO7HA5jk"
   },
   "source": [
    "# K_Means Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9TBSHWfgWxU",
    "outputId": "8d4ab613-7513-4732-bf06-8b7e7cd8a2c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to write your Kmeans algorithm\n",
    "#implement your KMeans algorithm here, and visualise the clusters obtained \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=4).fit(X)\n",
    "kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61GQRof2wkKd"
   },
   "outputs": [],
   "source": [
    "#code for visualisation of clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2t9Cz6yuJmO"
   },
   "source": [
    "# Elbow method\n",
    "\n",
    "\n",
    "\n",
    "*   Try to understand how elbow method works\n",
    "*   Plot the graph between average distance and the number of clusters\n",
    "*   Use elbow method to find the optimal number of clusters, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW_stdJuub4s"
   },
   "outputs": [],
   "source": [
    "def elbow_method():\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jHeGh-xuuDc"
   },
   "source": [
    "# Silhouette Method\n",
    "\n",
    "\n",
    "*   Compute silhouette score varying the K number of clusters\n",
    "\n",
    "*   Plot the graph between silhoutte score and number of clusters \n",
    "\n",
    "*   Find the optimal number of clusters using silhouette method\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Report the optimal number of clusters you obtained from above two methods (elbow and silhouette)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKRJ4IBMuwfd"
   },
   "outputs": [],
   "source": [
    "def silhouette_score():\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEvLJxmnuCof"
   },
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzssThP1g_NN",
    "outputId": "27d005d2-7405-4a81-a644-f35bdd3ef4f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to write your Kmeans algorithm\n",
    "#implement your KMeans algorithm here, and visualise the clusters obtained \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def AgglomerativeClustering():\n",
    "  clustering = AgglomerativeClustering().fit(visuals)\n",
    "  clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whf5OLe2wf6o"
   },
   "outputs": [],
   "source": [
    "#code for visualisation of clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdNC7XxxwuJW"
   },
   "source": [
    "# Dendogram\n",
    "\n",
    "\n",
    "*   Try to understand the difference between agglomerative clustering and hierarchical clustering\n",
    "*   Plot dendograms for both kinds of clustering\n",
    "*   Find the optimal number of clusters with the help of Dendogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NWavQn0hW3i"
   },
   "outputs": [],
   "source": [
    "# code to write dendogram\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3On7lpA11eB"
   },
   "source": [
    "# useful links to understand BERT\n",
    "\n",
    "*  https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270#:~:text=How%20BERT%20works,%2Dwords)%20in%20a%20text.&text=As%20opposed%20to%20directional%20models,sequence%20of%20words%20at%20once.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q_Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
